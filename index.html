<!DOCTYPE HTML>
<html>
    <head>
        <title>Learning to Rearrange Deformable Cables, Fabrics, and Bags with Goal-Conditioned Transporter Networks</title>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=1000">
        <link rel="stylesheet" href="https://use.typekit.net/quv7bsd.css"> <!-- fonts -->
        <link rel="stylesheet" href="style.css" />
        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
          ga('create', 'UA-89797207-1', 'auto');
          ga('send', 'pageview');
        </script>
    </head>
    <body id="body">
        <div id="main">
            <header id="header">
            </header>
            <!-- style="padding-bottom:1em" -->
            <div id="profile">
                <!-- <img src="images/profile.jpg"> -->
                <div id="profile-desc">
                    <div id="profile-name">Learning to Rearrange Deformable Cables, Fabrics, and Bags with Goal-Conditioned Transporter Networks
                        <p><br>International Conference on Robotics and Automation, 2021</p>
			<p><a href="https://github.com/DanielTakeshi/deformable-ravens">Code (GitHub)</a> &nbsp &nbsp <a href="https://arxiv.org/abs/2012.03385">Paper (arXiv)</a><p>
                    </div>
                    <p>
                        <b>Abstract</b>. Rearranging and manipulating deformable objects such as cables, fabrics, and bags is a long-standing challenge in robotic manipulation. The complex dynamics and high-dimensional configuration spaces of deformables, compared to rigid objects, make manipulation difficult not only for multi-step planning, but even for goal specification. Goals cannot be as easily specified as rigid object poses, and may involve complex relative spatial relations such as "place the item inside the bag". In this work, we develop a suite of simulated benchmarks with  1D, 2D, and 3D deformable structures, including tasks that involve image-based goal-conditioning and multi-step deformable manipulation.  We propose embedding goal-conditioning into Transporter Networks, a recently proposed model architecture for robotic manipulation that uses learned template matching to infer displacements that can represent pick and place actions.  We demonstrate that goal-conditioned Transporter Networks enable agents to manipulate deformable structures into flexibly specified configurations without test-time visual anchors for target locations. We also significantly extend prior results using Transporter Networks for manipulating deformable objects by testing on tasks with 2D and 3D deformables.
                    </p>
                </div>
                <div style="clear: both;"></div>
            </div>


            <!-- Updates
            It's kind of hard to get nice bullet points due to the way the website is formatted.
            -->
            <div class="section paper">
                <h1>Website Updates</h1>
                 <!--<ol style="list-style-type:square"> -->
                 <ol>
                  <li><p>03/01/2023: Added clarification on simulation results and a bar chart.</p></li>
                  <li><p>02/24/2023: Added four more physical experiment videos and an evaluation metric.</p></li>
                  <li><p>02/22/2023: The physical experiment videos have been improved for visual clarity.</p></li>
                  <li><p>02/21/2023: <b>We've added physical cable manipulation experiments</b>. We are actively adding more.</p></li>
                  <li><p>10/23/2022: Added some more examples of goal-conditioning for Cable-Line-Notarget.</p></li>
                  <li><p>05/30/2021: Finalized the website for the ICRA 2021 paper.</p></li>
                </ol>
                <br>
            </div>

            <div style="clear: both;"></div>
            <div class="divider"></div>



            <!-- The paper. Put the main part and the supplement here.

            To adjust height and other stuff for images in line:
            https://stackoverflow.com/questions/10270075/image-height-and-width-not-working
            Otherwise, the CSS style will take over by default.

            For centering, use the stuff I normally do with BAIR Blog posts, for example:
            https://www.computerhope.com/issues/ch001613.htm
            -->
            <div class="section paper">
                <h1>Paper</h1>
                <p>Latest version (March 26, 2021): (<a href="https://arxiv.org/abs/2012.03385">arXiv link here</a>). The arXiv link is the latest version and includes the supplementary material. We are planning to do another update in mid-2023 which will reflect some newer updates present on this website.</p><br>
                <p style="text-align:center;">
                <a href="https://drive.google.com/file/d/1YTg0fAPjgLGG1OXIAqgHu1MRgoakZ_X0/view?usp=sharing"><img src="images/paper-v01.png" style="width: 70%; class=center"></a>
                </p>
            </div>


            <!-- The awesome team. I need a better picture. -->
            <div class="divider"></div>
            <div class="section team">
                <h1>Team</h1>
                <div class="people-profile">
                    <a href="https://cs.cmu.edu/~dseita"><img src="images/people/daniel.jpg"><p>Daniel Seita</p></a>
                </div>
                <div class="people-profile">
                    <a href="http://www.peteflorence.com/"><img src="images/people/pete.jpg"><p>Pete Florence</p></a>
                </div>
                <div class="people-profile">
                    <a href="https://jonathantompson.github.io/"><img src="images/people/jonathan_tompson.jpeg"><p>Jonathan Tompson</p></a>
                </div>
                <div class="people-profile">
                    <a href="https://twitter.com/erwincoumans"><img src="images/people/erwin.jpg"><p>Erwin Coumans</p></a>
                </div>
                <div class="people-profile">
                    <a href="https://vikas.sindhwani.org/"><img src="images/people/vikas.jpg"><p>Vikas Sindhwani</p></a>
                </div>
                <div class="people-profile">
                    <a href="https://goldberg.berkeley.edu/"><img src="images/people/ken-1.jpg"><p>Ken Goldberg</p></a>
                </div>
                <div class="people-profile">
                    <a href="https://andyzeng.github.io/"><img src="images/people/andy.jpg"><p>Andy Zeng</p></a>
                </div>
                <div style="clear: both;"></div>
            </div>


            <!-- Overall Summary Video.
            Amazingly. www.youtube.com is needed instead of youtu.be ??
            https://scholarsark.com/question/what-is-the-difference-between-youtube-com-and-youtu-be/
            There should be no difference?

            Also, pretty sure the div style clear; both is needed to properly break out of above videos.
            -->
            <!-- <div class="divider"></div> -->
            <div style="clear: both;"></div>
            <div class="divider"></div>

            <div class="section video">
                <h1>3-Minute Summary Video (with Captions)</h1>
                <iframe width="910" height="512" src="https://www.youtube.com/embed/WLebiJDP6mg" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            </div>




            <!-- PHYSICAL EXPERIMENTS!!
            A huge relief. :)

            Also this took a while but I now realize we have the video in the css, so we can just
            use that div class and it seems to have default parameters that work well in the css,
            so I don't have to customize the style="" stuff.
            -->
            <div style="clear: both;"></div>
            <div class="divider"></div>
            <div class="section paper">
                <h1>Physical Experiments</h1>
                <p><b>Context:</b> we originally wrote this paper in 2020 during the middle of COVID-19, and due to the circumstances, we (virtually) presented our ICRA 2021 paper with simulation-only expeirments. In early 2023, we returned to this project to demonstrate how Goal-Conditioned Transporter Networks could be applied on physical hardware. Below, we show experiments inspired by the "Cable-Line-Notarget" task in our DeformableRavens simulation benchmark. For the cable, we <a href="https://www.amazon.com/gp/product/B07BMD5ZHJ/ref=ppx_yo_dt_b_asin_title_o00_s00?ie=UTF8&th=1">use this chain</a> with 13mm balls attached together, painted with a uniform color.</p><br>

                <p><b>Experiment Details:</b> we collect 30 demonstrations and use 24 of them for training (with the other 6 to monitor validation loss). We also collect 20 separate images of the cable on the workspace to be used as "goal images" at test time. For test-time execution, we use the snapshot after 20K iterations of training GCTN, which is the same as used in simulation, where one "iteration" is a batch size of 1 item. We use a Franka robot and move it using <a href="https://github.com/iamlab-cmu/frankapy">frankapy</a>. The Franka uses a mounted Azure Kinect camera on its end-effector; it returns to a top-down home pose after each action to query updated images of the workspace. We do <i>not</i> use specialized grippers. To initialize the setup, the human operator lightly tosses the cable on the workspace. We limit the number of robot actions to 10 per episode (i.e., rollout).</p><br>

                <p><b>Changes from Simulation:</b> enabling GCTNs for physical cable manipulation requires a few adjustments from simulation. Perhaps most critically, we cannot assume our gripper perfectly grasps the deformable. As a heuristic, we take a local crop centered at the a picking point and compute the best fit tangent line, and then have the gripper's finger tips open in the direction perpendicular to that tangent line. Second, to ease training, we currently use a binary segmentation mask as our images, instead of RGB and depth.</p><br>

                <p><b>Quantitative Evaluation Metric</b></b>: we report <b>cable mask intersection over union (IoU)</b>. We compute this at each time by considering the current and goal cable mask images (which are passed as input to the GCTN) and computing the IoU. We consider a test-time episode rollout as a success if the cable mask overlap ever reaches above some threshold within the 10-action limit. Our threshold is currently 0.25, which we found to correlate with strong qualitative results but which is also forgiving of certain physical limitations. For example the robot will have some imprecision when picking and placing due to slight errors in calibration.</p><br>

                <p><i>We will add more details about all the above points in the next edition of the paper on arXiv (exact timing TBD)</i>.</p>

                <h1>Videos of the Trained GCTN at Test Time</h1>

                <p>All videos here are test-time rollouts from the same GCTN policy trained on 24 demonstrations, and are at <b>4X speed</b>. In all the videos, we overlay (to the right) the current image of the workspace, and below it, the goal image. The current image changes after each action while the goal image <i>remains fixed</i>. For visual clarity we overlay RGB images in the videos, but the GCTN takes as input the current and goal cable binary mask images.</p><br>

                <p>The following two videos show successful examples of rollouts. The GCTN was able to increase the cable mask IoU from 0.0 to 0.278 in the first video, and from 0.0 to 0.308 in the second video.</p><br>

                <div class="video">
                    <!-- Rollout 008 with image overlay -->
                    <!--
                    <iframe width="420" height="230" src="https://www.youtube.com/embed/TFBMKtEMNu8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
                    -->
                    <iframe width="420" height="230" src="https://www.youtube.com/embed/DeJC2bTq4U0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
                    <!-- Rollout 009 with image overlay -->
                    <!--
                    <iframe width="420" height="230" src="https://www.youtube.com/embed/pxfMWx7mZ58" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
                    -->
                    <iframe width="420" height="230" src="https://www.youtube.com/embed/r73I5d9EEo4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
                </div>

                <p>The following two videos show two more successful rollouts. The GCTN was able to increase the cable mask IoU from 0.0 to 0.454 in the first video, and from 0.052 to 0.417 in the second video. The second video shows that the policy has a few suboptimal actions, but is able to recover.</p><br>

                <div class="video">
                    <!-- Rollout 017 with image overlay -->
                    <iframe width="420" height="230" src="https://www.youtube.com/embed/TOz2AKct7Fc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
                    <!-- Rollout 019 with image overlay -->
                    <iframe width="420" height="230" src="https://www.youtube.com/embed/4advSJ6Zwq0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
                </div>

                <p>Here are two more successful rollouts (as judged by cable mask IoU). The GCTN was able to get the cable mask IoU from 0.083 to 0.313, and from 0.048 to 0.265 in these two videos. In the first video, it also shows that the GCTN is able to manipulate the cable even when it is partially outside the workspace (see 1:00) which did not happen in the training data.</p><br>

                <div class="video">
                    <!-- Rollout 021 with image overlay -->
                    <iframe width="420" height="230" src="https://www.youtube.com/embed/_2W8ZwLgmm8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
                    <!-- Rollout 022 with image overlay -->
                    <iframe width="420" height="230" src="https://www.youtube.com/embed/PlDXxP4NGD0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
                </div>

                <p>The following two videos demonstrate the challenge of precisely getting a cable to align with a goal. Even though the robot might qualitatively move the cable to a "reasonable" goal configuration, the cable mask IoU only gets to 0.131 and 0.013 at the end of these respective videos.</p><br>

                <div class="video">
                    <!-- Rollout 012 with image overlay -->
                    <!--
                    <iframe width="420" height="230" src="https://www.youtube.com/embed/rdvYNfsQwHw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
                    -->
                    <iframe width="420" height="230" src="https://www.youtube.com/embed/rOT148Z5ESs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
                    <!-- Rollout 013 with image overlay -->
                    <!--
                    <iframe width="420" height="230" src="https://www.youtube.com/embed/ipRbXzo_OQU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
                    -->
                    <iframe width="420" height="230" src="https://www.youtube.com/embed/mufqPYUiErU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
                </div>

                <p><b>Limitations:</b> First, there can be grasp failures, particularly when the cable self-overlaps, as shown in the first video below (0:36 to 0:38). Interestingly, the GCTN model was still able to make the cable end up in a nice configuration as the next time step resulted in a different picking (and thus placing) point, and the final cable mask IoU is 0.333 which exceeds our success cutoff. Second, some failures are due to poor predictions from the GCTN model. For example, the second video (which has a challenging starting cable) shows a 10-action sequence when the model picks at suboptimal points (e.g., at 1:06) or place at the "wrong" cable endpoint location (e.g., at 1:56) in addition to a grasping failure at 0:06.</p><br>

                <div class="video">
                    <!-- Rollout 006 with image overlay -->
                    <!--
                    <iframe width="420" height="230" src="https://www.youtube.com/embed/kAPRXr6LAM4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
                    -->
                    <iframe width="420" height="230" src="https://www.youtube.com/embed/RjhsX_OR3gQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
                    <!-- Rollout 014 -->
                    <!--
                    <iframe width="420" height="230" src="https://www.youtube.com/embed/A54XYMO_RuE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
                    -->
                    <iframe width="420" height="230" src="https://www.youtube.com/embed/_QkRjrVgosA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
                </div>

            </div>


            <!-- Highlights. :D
            General outline: put all the tasks here, along with all demonstration videos.
            Make sure the GIFs don't take up too much space. Cut down file sizes if needed.
            -->
            <div class="divider"></div>

            <div class="section part 1">
                <h1>Videos -- Scripted Demonstrations</h1>
                <p>These are screen recordings of the scripted demonstrator policy for tasks in the DeformableRavens simulation benchmark. The bag tasks have screen recordings slightly sped up, and we compress some GIFs to reduce file sizes.</p><br>
                <div class="highlight-proj">
                    <img src="videos/tasks/demos-cable-ring-1x.gif" width="100%">
                    <p>Cable-Ring</p>
                </div>
                <div class="highlight-proj">
                    <img src="videos/tasks/demos-cable-ring-notarget-1x.gif" width="100%">
                    <p>Cable-Ring-Notarget</p>
                </div>
                <div class="highlight-proj">
                    <img src="videos/tasks/demos-cable-shape-seed00-1x.gif" width="100%">
                    <p>Cable-Shape</p>
                </div>
            </div>

            <div class="section part 2">
                <div class="highlight-proj">
                    <img src="videos/tasks/demos-cable-shape-notarget-00.gif" width="100%">
                    <p>Cable-Shape-Notarget</p>
                </div>
                <div class="highlight-proj">
                    <img src="videos/tasks/demos-cable-line-notarget-1x.gif" width="100%">
                    <p>Cable-Line-Notarget</p>
                </div>
                <div class="highlight-proj">
                    <img src="videos/tasks/demos-cloth-cover-multiple-episodes-1x-reduced.gif" width="100%">
                    <p>Fabric-Cover</p>
                </div>
            </div>

            <div class="section part 3">
                <div class="highlight-proj">
                    <img src="videos/tasks/demos-cloth-flat-1x.gif" width="100%">
                    <p>Fabric-Flat</p>
                </div>
                <div class="highlight-proj">
                    <img src="videos/tasks/demos-cloth-flat-notarget-00-1x.gif" width="100%">
                    <p>Fabric-Flat-Notarget</p>
                </div>
                <div class="highlight-proj">
                    <img src="videos/tasks/demos-bag-alone-open-4x-reduced.gif" width="100%">
                    <p>Bag-Alone-Open</p>
                </div>
            </div>

            <div class="section part 4">
                <div class="highlight-proj">
                    <img src="videos/tasks/demos-bag-items-1-4x-reduced.gif" width="100%">
                    <p>Bag-Items-1</p>
                </div>
                <div class="highlight-proj">
                    <img src="videos/tasks/demos-bag-items-2-4x-reduced-further.gif" width="100%">
                    <p>Bag-Items-2</p>
                </div>
                <div class="highlight-proj">
                    <img src="videos/tasks/demos-bag-color-goal-01-4x-reduced.gif" width="100%">
                    <p>Bag-Color-Goal</p>
                </div>
            </div>


            <!-- Highlights. :D

            Now put the learned policies here. Show successful cases, but also failure cases.
            As before, make sure the GIFs don't take up too much space. Cut down file sizes if needed.

            <div class="divider"></div>
            Note: the "clear: both;" thing is to get newlines done correctly.
            Actually let's do failure cases after this. Failures can happen with both trained and scripted policies.
            -->

            <div style="clear: both;"></div>

            <div class="section part 1">
                <h1>Videos -- Learned Policies (Bag Tasks)</h1>
                <p>These are screen recordings of learned policies when deployed on test-time starting configurations. For some GIFs, to speed them up and reduce file sizes, we remove frames corresponding to pauses between pick and place actions.</p><br>

                <div class="highlight-proj">
                    <img src="videos/bag-items-1-transporter-100-success-3-actions-Sept11-reduced-50perc.gif" width="100%">
                    <p>Bag-Items-1. (Zoomed-in) Transporter trained on 100 demos. It successfully opens the bag, inserts the cube, and brings the bag to the target.</p>
                </div>
                <div class="highlight-proj">
                    <img src="videos/bag-items-2-transporter-1000-0-test-01-success-reduced-50perc.gif" width="100%">
                    <p>Bag-Items-2. Transporter trained on 1000 demos. It successfully opens the bag, inserts both blocks, and brings the bag to the target.</p>
                </div>
                <div class="highlight-proj">
                    <img src="videos/bag-color-goal-transporter-goal-10-0-test-19-success-3x.gif" width="100%">
                    <p>Bag-Color-Goal. Transporter-Goal-Split trained on 10 demos. The goal image (not shown above) shows the item in the red bag, in which the policy correctly inserts the block.</p>
                </div>

                <div class="highlight-proj">
                    <img src="" width="100%">
                    <p></p>
                </div>
            </div>

            <div style="clear: both;"></div>

            <div class="section part 1">
                <h1>Videos -- Goal Conditioning in Depth (Cable-Line-Notarget)</h1>
                <p>Test-time rollouts (on Cable-Line-Notarget) of one GCTN policy trained on 1000 demonstrations. The cables start from the <i>same</i> starting configuration, and the only thing that changes is the goal image. This shows the utility of a goal-conditioned policy.</p><br>
                <p>For clarity, we show the goal images below each of the videos. The goal images consist of two parts: the scene as viewed from the front camera (to the left, purely for visualization) and the top-down RGB image (to the right) which is actually supplied to the GCTN as input at each time step. (GCTN also uses the corresponding top-down depth goal image as input.) The GCTN policy achieved a success in these three examples by triggering the tolerance threshold to be close to the goal.</p><br>

                <div class="highlight-proj">
                    <img src="videos/GCTN-cable-shape-notarget-goal-00.gif" width="100%">
                    <img src="images/GCTN-goal-00.png" width="100%">
                    <p>Cable-Line-Notarget, Goal 1.</p>
                </div>
                <div class="highlight-proj">
                    <img src="videos/GCTN-cable-shape-notarget-goal-01.gif" width="100%">
                    <img src="images/GCTN-goal-01.png" width="100%">
                    <p>Cable-Line-Notarget, Goal 2.</p>
                </div>
                <div class="highlight-proj">
                    <img src="videos/GCTN-cable-shape-notarget-goal-03.gif" width="100%">
                    <img src="images/GCTN-goal-03.png" width="100%">
                    <p>Cable-Line-Notarget, Goal 3.</p>
                </div>

                <div class="highlight-proj">
                    <img src="" width="100%">
                    <p></p>
                </div>
            </div>

            <div style="clear: both;"></div>

            <div class="section part 1">
                <h1>Videos -- Limitations and Failure Cases</h1>
                <p>These are screen recordings showing some informative failure cases, which may happen with scripted demonstrators or with learned policies. These motivate some interesting future work directions, such as learning policies that can explicitly recover from failures.</p><br>

                <div class="highlight-proj">
                    <img src="videos/demo-bag-items-1-cube-falls-out-3x-reduced.gif" width="100%">
                    <p>Bag-Items-1. Possibly the most common failure case. Above is the scripted policy, but this also occurs with learned Transporters. The cube is at a reasonable spot, but falls out when the robot attempts to lift it.</p>
                </div>
                <div class="highlight-proj">
                    <img src="videos/bag-items-1-transporter-10-interesting-failure-Sept16-spedup-reduced.gif" width="100%">
                    <p>Bag-Items-1. Failure with a learned Transporter policy (trained with 10 demos). The policy repeatedly attempts to insert the cube in the bag but fails, and (erroneously) brings an empty bag to the target.</p>
                </div>
                <div class="highlight-proj">
                    <img src="" width="100%">
                    <p></p>
                </div>
            </div>

            <div style="clear: both;"></div>

            <div class="section part 1">
                <h1>Quantitative Results (Updated 03/01/2023)</h1>
                <p>In early 2023, we found a bug in how we were processing the goal image data into both variants of goal-conditioned Transporters ("Transporter-Goal-Stack" and "Transporter-Goal-Split"). This bug does not affect any of the other baselines, nor does it affect the vanilla Transporter networks. Fixing the bug results in improved performance for GCTNs for most of the tasks and variants. For fairness, we also retrained the baselines of GT-State MLP and GT-State MLP (2-step), which got similar results as earlier. Below are bar charts showing the updated performance results for 1 and 10 demonstrations. We also see that the baseline version of "Transporter-Goal-Stack" is slightly better now. For clarify we can refer to "GCTNs" as referring to either of the two goal-conditioned Transporter models. <i>In the below charts, GCTN (v1) refers to Transporter-Goal-Stack, and GCTN (v2) refers to Transporter-Goal-Split.</i> We plan to update the arXiv version soon with these results.</p><br>

                <!-- Yes I know I am using the video class here... -->
                <div class="video">
                    <img src="images/plot_ICRA2021_goal_conditioned_0001.png" width="48%">
                    <img src="images/plot_ICRA2021_goal_conditioned_0010.png" width="48%">
                </div>
                <br>
            </div>



            <!-- The code.
            Let's not put any code commands on this webpage. Easier to do it on GitHub.
            -->
            <div style="clear: both;"></div>
            <div class="divider"></div>
            <div class="section paper">
                <h1>Code</h1>
                <p>Here is the GitHub link: <a href="https://github.com/DanielTakeshi/deformable-ravens">https://github.com/DanielTakeshi/deformable-ravens</a>. If you have questions, please use the public issue tracker. I will try to actively monitor the issue reports, though I cannot guarantee a response.</p><br>
            </div>

            <!-- Demonstration Data.
            Let's link to a bunch of demonstration data.
            -->
            <div class="divider"></div>
            <div class="section paper">
                <h1>Demonstration Data</h1>
                <p>These are zipped files that contain demonstration data of 1000 episodes. These are used to train policies.</p><br>
                <ol>
                    <li><p>Cable-Ring           --- (<a href="https://drive.google.com/file/d/1kMnTFIAGq1_nR2bjsNAXzoa6Mgh-vuiV/view?usp=sharing">LINK (4.0G)</a>) </p></li>
                    <li><p>Cable-Ring-Notarget  --- (<a href="https://drive.google.com/file/d/1HCAG0Tuo6p6YLoWBqfPAS86u-MaDTZP7/view?usp=sharing">LINK (3.9G)</a>) </p></li>
                    <li><p>Cable-Shape          --- (<a href="https://drive.google.com/file/d/1blsL0VtgH1Ux_-qXH9Pa7Mq2uQO55ZPW/view?usp=sharing">LINK (4.1G)</a>) </p></li>
                    <li><p>Cable-Shape-Notarget --- (<a href="https://drive.google.com/file/d/1W60GtNlAeFpos-7eC_UyFYE_6qUIYXDn/view?usp=sharing">LINK (4.1G)</a>) </p></li>
                    <li><p>Cable-Line-Notarget  --- (<a href="https://drive.google.com/file/d/1cDLsTBNucC88-R6pU34YeS-YINhiung-/view?usp=sharing">LINK (3.3G)</a>) </p></li>
                    <li><p>Fabric-Cover         --- (<a href="https://drive.google.com/file/d/1VQidyV7hshy5gWjI9mXokZeMVqC5XNp-/view?usp=sharing">LINK (1.6G)</a>) </p></li>
                    <li><p>Fabric-Flat          --- (<a href="https://drive.google.com/file/d/12oP9pH45Eswmz7gKI5AbZUZeRymTckkC/view?usp=sharing">LINK (2.2G)</a>) </p></li>
                    <li><p>Fabric-Flat-Notarget --- (<a href="https://drive.google.com/file/d/14EfxswRbBphWCE0Z9r442TmOFGUiPGXv/view?usp=sharing">LINK (2.2G)</a>) </p></li>
                    <li><p>Bag-Alone-Open       --- (<a href="https://drive.google.com/file/d/1HkgSBINM5_KRRqE_XlnbHv4l04NaWrQk/view?usp=sharing">LINK (2.5G)</a>) </p></li>
                    <li><p>Bag-Items-1          --- (<a href="https://drive.google.com/file/d/1iplau-dIH5i1IPkphVwDCy-mJ67Wc2yv/view?usp=sharing">LINK (2.2G)</a>) </p></li>
                    <li><p>Bag-Items-2          --- (<a href="https://drive.google.com/file/d/1952cbN3Hef8FMcCpUjI84Y8cOyY0644Z/view?usp=sharing">LINK (2.8G)</a>) </p></li>
                    <li><p>Bag-Color-Goal       --- (<a href="https://drive.google.com/file/d/1OXXPV1jNApK2sxnpCK-DR_X_4BtoP5QZ/view?usp=sharing">LINK (2.2G)</a>) </p></li>
                    <li><p>Block-Notarget       --- (<a href="https://drive.google.com/file/d/1IXU3QrdRuevuyfmMhjKkHz9pSV9iLk1G/view?usp=sharing">LINK (1.0G)</a>) </p></li>
                </ol>
                <br><p>These are zipped files that contain demonstration data for 20 goals. These are only used for the goal-conditioned cases to ensure evaluation is done in a reasonably consistent manner.</p><br>
                <ol>
                    <li><p>Cable-Shape-Notarget --- (<a href="https://drive.google.com/file/d/15IRsCnEMBbZu4ZU_-uQ7Qz-MdUxMuU-A/view?usp=sharing">LINK</a>) </p></li>
                    <li><p>Cable-Line-Notarget  --- (<a href="https://drive.google.com/file/d/1tnobjeDKlNinWJHSa7l8b_GJnQCk-ksO/view?usp=sharing">LINK</a>) </p></li>
                    <li><p>Fabric-Flat-Notarget --- (<a href="https://drive.google.com/file/d/1loeKuMJBxjDEcWukW3Xfcoqd3noZTO7d/view?usp=sharing">LINK</a>) </p></li>
                    <li><p>Bag-Color-Goal       --- (<a href="https://drive.google.com/file/d/1udqmSRQfW0JSXpIkbc0vd-fulJOdKVOM/view?usp=sharing">LINK</a>) </p></li>
                    <li><p>Block-Notarget       --- (<a href="https://drive.google.com/file/d/19OwWip5vhczw3TiVnl_iCZ6guu9u8uJD/view?usp=sharing">LINK</a>) </p></li>
                </ol>
                <br><p>To unzip, run <b>tar -zvxf [filename].tar.gz</b>. Some of the data files will unzip to different file names, since we changed some task names for the purpose of the paper (while keeping the code and data with the original task names). Specifically, (1) the three "fabric" tasks are referred to as "cloth", (2) "bag-items-1" and "bag-items-2" are referred to as "bag-items-easy" and "bag-items-hard", and (3) "block-notarget" is referred to as "insertion-goal".</p><br>
                <p>The demonstration data should be zipped to <b>data/</b> and the goals data should be zipped to <b>goals/</b>.</p><br>
            </div>


            <!-- BibTex -->
            <div class="divider"></div>
            <div class="section bibtex">
                <h1>BibTeX</h1>
                <p>
                <pre><span style="font-size: 12px; font-family: courier new, courier, monospace">
@inproceedings{seita_bags_2021,
    author    = {Daniel Seita and Pete Florence and Jonathan Tompson and Erwin Coumans and Vikas Sindhwani and Ken Goldberg and Andy Zeng},
    title     = {{Learning to Rearrange Deformable Cables, Fabrics, and Bags with Goal-Conditioned Transporter Networks}},
    booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
    Year      = {2021}
}
                </pre>
                </p>
            </div>


            <!-- Acknowledgments -->
            <div class="divider"></div>
            <div class="section acknowledgements">
                <h1>Acknowledgements</h1>
                <p>Daniel Seita is supported by the Graduate Fellowships for STEM Diversity (<a href="https://stemfellowships.org/">website</a>). We thank Xuchen Han for assistance with deformables in PyBullet, and Julian Ibarz for helpful feedback on writing.</p><br><br>
            </div>

        </div>
    </body>
</html>
